\documentclass[12pt]{extarticle}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{color}
\usepackage{hyperref}

\usepackage[margin=0.5in]{geometry}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\usepackage{collectbox}
\DeclarePairedDelimiter\ceil{\lceils}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage{amsthm}
%opening
\title{Machine Learning}
\author{Daniel Drake}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section] 

\theoremstyle{Definition}
\newtheorem{def.}{Definition}[section] 

\theoremstyle{Definition}
\newtheorem{prf}{Proof}[section] 

\theoremstyle{plain}
\newtheorem{exmp}{Example}[section]

\begin{document}
		\maketitle			
\section{Change}
\begin{def.} \textbf{Metric} \\ 
	Let X be a non-empty set.. \\ 
	Let $d : X \times X \to \mathbb{R}^+_0$ such that: 
	\begin{itemize}
		\item $(\forall x \in X) d(x,x) = 0$
		\item $(\forall x,y \in X) d(x,y) = 0 \Leftrightarrow x = y$ 
		\item $(\forall x,y \in X) d(x,y) = d(y,x)$
		\item $(\forall x,y,z \in X) d(x,z) \leq d(x,y) + d(y,z)$
	\end{itemize}	
	Then $d$ is called a metric and $(X,d)$ is called a metric space. \\
	\href{https://en.wikipedia.org/wiki/Metric_space}{Reference}
\end{def.}
\begin{def.} \textbf{Limit of a function} \\ 
	Let $T : X \to Y$ where $(X,d_X)$ and $(Y,d_Y)$ are metric spaces. \\ 
	Then fix $x_0 \in X$. \\
	If:
	$$(\exists L \in Y)( \forall \epsilon > 0 )(\exists \delta > 0)(\forall x \in X)(d(x,x_0) < \delta \Rightarrow d(f(x),L) < \epsilon)$$
	Then: $$\lim_{x \to x_0} f(x) = L$$ 
	\href{https://en.wikipedia.org/wiki/Limit_of_a_function}{Reference}
\end{def.}
\begin{def.} \textbf{Derivative} \\ 
	Let $\hat{f} : \mathbb{R} \to \mathbb{R}$ \\ 
	Further let $f = \hat{f}|_U$ where $U \in \tau_\mathbb{R}$ \\ 
	Then $f$ is said to be differentiable at $x \in U$ if there exists an $L_x$ such that: \\ 
	$$L_x = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}$$
	If $L_x$ exists for all $x \in U$ then we write: \\ 
	$$\frac{d}{dx}f(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}$$
	\href{https://en.wikipedia.org/wiki/Differentiable_function}{Reference}
\end{def.}
\begin{def.} \textbf{Partial Derivative} \\
	Let $\hat{f} : \mathbb{R}^n \to \mathbb{R}$ \\
	Further let $f = \hat{f}|_U$ where $U \in \tau_{\mathbb{R}^n}$ \\ 
	Then $f$ is said to be differentiable at $x \in U$ with respect to the i'th component of $x$ if there exists an $L_{x_i}$ such that: \\  
	$$L_{x_i} = \lim_{h \to 0} \frac{f(x_1,...,x_i + h,...,x_n) - f(x_1,...,x_i,...,x_n)}{h}$$
	If $L_{x_i}$ exists for all $x \in U$ then we write: \\ 
	$$\frac{\partial}{\partial x_i} f(x) = \lim_{h \to 0} \frac{f(x_1,...,x_i + h,...,x_n) - f(x_1,...,x_i,...,x_n)}{h}$$ 
	\href{https://en.wikipedia.org/wiki/Partial_derivative}{Reference}
\end{def.}
\begin{def.} \textbf{Differentiability of a multi-variable function.} \\
	Let $\hat{f} : \mathbb{R}^m \to \mathbb{R}^n$ such that: \\ 
	\[
	\hat{f}(x) = 
	\begin{bmatrix}
	f_1(x) \\ 
	\vdots \\
	f_n(x) \\
	\end{bmatrix}
	\text{ and } 
	(\forall j \in \mathbb{N}_n)(f_j : \mathbb{R}^m \to \mathbb{R})
	\]
	Further let $f = \hat{f}|_U$ where $U \in \tau_{\mathbb{R}^m}$ \\ 
	Then $f$ is said to be differentiable at $x \in U$ if there exists a linear operator $J_f: \mathbb{R}^m \to \mathbb{R}^n$ such that: \\ 
	$$\lim_{h \to \vec{0}} \frac{||f(x + h) - f(x) + J_f(h)||_{\mathbb{R}^m}}{||h||_{\mathbb{R}^n}} = 0$$
	\href{https://en.wikipedia.org/wiki/Differentiable_function}{Reference}
\end{def.}
\begin{thm} \textbf{If a multi-variable function, f, is differentiable at x then the linear operator J is the Jacobian matrix.} \\ 
	So our guess is that: \\ 
	\[
	J_f = 
	\begin{bmatrix}
	\frac{\partial}{\partial x_1} f_1(x) & \cdots & \frac{\partial}{\partial x_n} f_1(x) \\ 
	\vdots & \ddots & \vdots \\ 
	\frac{\partial}{\partial x_1} f_m(x) & \cdots & \frac{\partial}{\partial x_n} f_m(x) \\ 
	\end{bmatrix}
	\]
	since this form is a linear operator mapping from the appropriate space to the appropriate space. It should be noted that the transpose of this matrix can not satisfy the definition of differentiability of a multi-variable function and so it is not the correct linear operator. 
\end{thm}
\begin{def.} \textbf{Gradient} \\ 
	Let $f : \mathbb{R}^n \to \mathbb{R}$ be a Differentiable function. \\ 
	Then $\nabla f: \mathbb{R}^n \to \mathbb{R}^n$ where: \\ 
	\[
	\nabla f(x) := 
	\begin{bmatrix}
	\frac{\partial }{\partial x_1}f(x) \\ 
	\vdots \\
	\frac{\partial }{\partial x_n}f(x) \\ 				
	\end{bmatrix}
	\forall x \in \mathbb{R}^n
	\]
	is called the Gradient of f. \\
	\href{https://en.wikipedia.org/wiki/Gradient}{Reference}
\end{def.}
\begin{def.} \textbf{Derivative with respect to a vector and the subspace gradient.} \\ 
	Let $X = \{X_j\}_{j=0}^{n-1}$ be a sequence of finite dimensional vector spaces where $dim(X_j) = k_j = m_j \times n_j$ \\ 
	Let $T : \prod_{j=0}^{n-1} X_j \to Y$ where $Y$ is a finite dimensional vector space with $dim(Y) = k_y$ \\ 
	Let $x \in X_j$ for some $j \in \{0,...,n-1\}$ \\ 
	Where 
	\[
	x = 
	\begin{bmatrix}
	x_{1,1} & \cdots & x_{1,n_j} \\ 
	\vdots & \ddots & \vdots \\
	x_{m_j,1} & \cdots & x_{m_j,n_j} \\ 	
	\end{bmatrix}
	\]
	\[
	D_x T(z) = 
	\begin{bmatrix}
	\frac{\partial}{\partial x_{1,1}} T_1(z) & \cdots & \frac{\partial}{\partial x_{1,n_j}} T_1(z) \\
	\vdots & \ddots & \vdots \\
	\frac{\partial}{\partial x_{m_j,1}} T_1(z) & \cdots & \frac{\partial}{\partial x_{m_j,n_j}} T_1(z) \\
	\vdots & \vdots & \vdots \\ 
	\frac{\partial}{\partial x_{1,1}} T_{k_y}(z) & \cdots & \frac{\partial}{\partial x_{1,n_j}} T_{k_y}(z) \\
	\vdots & \ddots & \vdots \\
	\frac{\partial}{\partial x_{m_j,1}} T_{k_y}(z) & \cdots & \frac{\partial}{\partial x_{m_j,n_j}} T_{k_y}(z) \\	
	\end{bmatrix}
	\]
\end{def.}
\begin{def.} \textbf{Product space Derivative} \\ 
		Let $X = \{X_j\}_{j=0}^{n-1}$ be a sequence of finite dimensional vector spaces where $dim(X_j) = k_j$ \\ 
		Let $T : \prod_{j=0}^{n-1} X_j \to Y$ where $Y$ is a finite dimensional vector space with $dim(Y) = k_y$ \\
		Let $\{x_j\}_{j=0}^{n-1}$ be a sequence of vectors such that: $(\forall j \in \{0,...,n-1\})(x_j \in X_j)$ \\ 
		The product space derivative at the point $z \in X$ is: \\ 
		\[
		D_{X} T(z) = 
		\begin{bmatrix}
		D_{x_0} T(z) \\ 
		\vdots \\ 
		D_{x_{n-1}} T(z) \\ 		
		\end{bmatrix}
		\]
\end{def.}


\newpage
\begin{def.} \textbf{Fréchet derivative} \\ 
		Let $V,W$ be normed vector spaces and $U \subset V$ be an open set. \\ 
		An operator $f : U \to W$ is said to be Fréchet differentiable if there exists a bounded linear operator $A : V \to W$ such that: \\ 
		$$\lim_{||h||\to0} \frac{||f(x+h) - f(x) + Ah||_W}{||h||_V} = 0$$
		\href{https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative}{Reference}
\end{def.} 
\begin{thm} \textbf{Fréchet derivative of a bounded linear operator} \\ 
	Let $V,W$ be normed vector spaces and $U \subset V$ be an open set. \\ 
	Let $\hat{f} : V \to W$ be a bounded linear operator. \\
	Then lets look at $f = \hat{f}|_{U}$ \\ 
	My guess is that $A = \hat{f}$ \\ 
	Let $x \in U$ and $h \in U \pitchfork ||h|| \not = 0$ and $x+h \in U$, Then: \\ 
	$$\frac{||f(x+h) - f(x) + Ah||_W}{||h||_V} = \frac{||f(x)+f(h) - f(x) + \hat{f}(h)||_W}{||h||_V} = \frac{||f(x)+f(h) - f(x) + f(h)||_W}{||h||_V}= 0$$
	Thus let $\epsilon > 0$ and $\delta > 0$ \\ 
	Then if $0 < ||h|| < \delta$ we know that  
	$\frac{||f(x+h) - f(x) + Ah||_W}{||h||_V} = 0 < \epsilon$ \\ 
	Therefore: 
	$$\lim_{||h||\to0} \frac{||f(x+h) - f(x) + Ah||_W}{||h||_V} = 0$$
	Thus $A = \hat{f}$ is the Fréchet derivative  of f. 
\end{thm}
\subsection{Finite Composition Operator}
\begin{def.} \textbf{Finite Composition Operator} \\ 
	Let the collection $X = \{X_j\}_{j=0}^{n}$ be a finite sequence of sets. \\ 
	Further let $\{T_j\}_{j=0}^{n-1}$ be a finite sequence of operators such that $(\forall j \in \mathbb{N}_{n-1})(T_j : X_j \to X_{j+1})$ \\ 
	Then $T^n : X_0 \to X_{n}$ defined by: 
	$$T^n := \bigcirc_{j=0}^{n-1} T_j$$ 
	is called the \textbf{Finite Composition Operator defined on X}. \\ 
\end{def.}
\begin{def.} \textbf{Multi-variable Finite Composition Iteration} \\ 
	Let the collection $X = \{X_j\}_{j=0}^n$ and $Y = \{Y_j\}_{j=0}^{n-1}$ be finite sequences of sets. \\ 
	Further let $\{T_j\}_{j=0}^{n-1}$ be a finite sequence of operators such that: $(\forall j \in \mathbb{N}_{n-1})(T_j : X_j \times Y_j \to X_{j+1})$ \\
	Let $T^n : X_0 \times \prod_{j=0}^{n-1}Y_j \to X_n$ where: \\ 
	$$T^n (x,y) = z_n \text{ where } z_{j+1} = T_j( z_{j},\pi_j (y) ) \text{ or } z_{j+1} = T_j( z_{j} )  \text{ and } z_0 = x \in X_0$$
\end{def.}
\begin{def.} \textbf{Gradient Descent} \\ 
Let $E : \mathbb{R}^n \to \mathbb{R}$ be a differentiable operator. \\ 
The method of Gradient Descent says that a local minimum of $E$ can be found using the following iteration: \\ 
$$a_{n+1} = a_n - \gamma \nabla E(a_n)$$
Where $\gamma > 0$ \\ 
\end{def.}
\begin{exmp} \textbf{Objective Operator for Data Set Defined Operator Approximation} \\ 		
Let $X \subset \mathbb{R}^n, Y \subset \mathbb{R}^m$ such that $X\times Y$ defines an operator T. \\ 
$$E(a) = \sum_{x \in X} ||T(x) - T^n(x,a)||$$ 
\end{exmp}
\newpage
\section{Surjective Continuous Non-decreasing Bounded Functionals}
	Let $B = \{f : \mathbb{R} \to [0,1] | f \text{ is surjective, continuous, and non-decreasing.}\}$ \\ 
	\begin{thm} \textbf{B is convex. } \\
			Let $f,g \in B$ and $h(x) := \lambda f(x) + (1-\lambda)g(x)$ where $\lambda \in [0,1]$ \\ 
			Then $h$ is still continuous since the linear combination of continuous functions is continuous.  \\ \\
			Since both f and g are surjective and non-decreasing, then there exists $x_0,y_0,x_1,y_1$ in $\mathbb{R}$ such that: \\ 
			$f(x_0) = 0 = g(y_0)$ and $f(x_1) = 1 = g(y_1)$ \\ 
			Suppose WLOG that $x_0 \leq y_0$ and $x_1 \leq y_1$ \\
			Then we know that: 
			$$h(x_0) = \lambda f(x_0) + (1-\lambda)g(x_0) = \lambda 0 + (1-\lambda)0 = 0$$
			and 
			$$h(y_1) = \lambda f(y_1) + (1-\lambda)g(y_1) = \lambda 1 + (1-\lambda)1 = 1$$
			Now if we pick $\alpha \in [0,1]$ by the intermediate value theorem, we know that there exists an $x_\alpha \in [x_0,y_1]$ such that: 
			$$h(x_\alpha) = \alpha$$
			Since $\alpha$ was arbitrary element, I have shown that $h$ is surjective. \\ \\
			Finally, let $x_0 < x_1$ be elements in $\mathbb{R}$ \\ 
			Then we know that $f(x_0) \leq f(x_1)$ and $g(x_0) \leq g(x_1)$ \\ 
			$\Rightarrow \lambda f(x_0) \leq \lambda f(x_1)$ and $(1-\lambda)g(x_0) \leq (1-\lambda)g(x_1)$ \\ 
			$\Rightarrow \lambda f(x_0) + (1-\lambda)g(x_0) \leq \lambda f(x_1) + (1-\lambda)g(x_1)$ \\ 
			$\Rightarrow h(x_0) \leq h(x_1)$ \\ 
			Thus $h$ is non-decreasing. \\ 
			Since h is surjective, continuous, and non-decreasing, then $h \in B$ \\ 
			Thus $B$ is convex. 
	\end{thm}
	\begin{thm} \textbf{B is translation invariant.} \\
			Let $f \in B$ and $g(x) := f(x+c)$ where $c \in \mathbb{R}$ \\
			f is continuous and so is the addition operator so g is continuous. \\
			Let $\alpha \in [0,1]$ since f is surjective then $\exists x \in \mathbb{R} \pitchfork f(x) = \alpha$ \\ 
			Then $g(x-c) = f(x+c-c) = f(x) = \alpha$ and so g is surjective. \\ 
			Let $x < y$ be elements in $\mathbb{R}$ \\
			Then $f(x) \leq f(y) \Rightarrow f(x+c) \leq f(y+c)$ \\ 
			$\Rightarrow g(x) \leq f(y)$ and so g is non-decreasing. \\ 
			Thus $g \in B$ and B is therefore translation invariant. 
	\end{thm}
	\begin{thm} \textbf{B is not complete.} \\
			
	\end{thm}
	\begin{thm} \textbf{Every element in B can be decomposed as a finite non-trivial convex combination from B} \\
	
	\end{thm}

\end{document}
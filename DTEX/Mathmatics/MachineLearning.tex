\documentclass[12pt]{extarticle}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{color}
\usepackage{hyperref}

\usepackage[margin=0.5in]{geometry}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\usepackage{collectbox}
\DeclarePairedDelimiter\ceil{\lceils}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage{amsthm}
%opening
\title{Analysis, Topology, Optimization, Machine Learning, and Computational Analysis}
\author{Daniel Drake}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section] 

\theoremstyle{plain}
\newtheorem{axiom}{Axiom}[section] 

\theoremstyle{plain}
\newtheorem{lma}{Lemma}[section] 

\theoremstyle{Definition}
\newtheorem{def.}{Definition}[section] 

\theoremstyle{Definition}
\newtheorem{prf}{Proof}[section] 

\theoremstyle{plain}
\newtheorem{exmp}{Example}[section]

\theoremstyle{plain}
\newtheorem{ruleOfInference}{Rule Of Inference}[section]

\newcommand{\cut}[0]{\noindent\framebox[\linewidth]{\rule{\linewidth}{2pt}}\\}
\newcommand{\prof}[0]{	\noindent \textbf{Proof:} \rule{500pt}{2pt} \\ }

\begin{document}
\maketitle			
\section{Notation, Set Theory, and Logic}
\cut
\begin{def.} \textbf{Common Sets of Numbers} \\ 
	$$\mathbb{N} = \{1,2,...\}$$	
	$$\mathbb{N}_0 = \{0,1,2,...\}$$
	$$\mathbb{N}_m = \{1,2,...,m\} \text{ where } m \in \mathbb{N}$$
	$$\mathbb{R} \text{ is the set of Real Numbers}$$
	$$\mathbb{R}^+ = \{x \in \mathbb{R} : x > 0\}$$
	$$\mathbb{R}_0^+ = \{x \in \mathbb{R} : x \geq 0\}$$
	$$\mathbb{R}^- = \{x \in \mathbb{R} : x < 0\}$$
	$$\mathbb{R}_0^- = \{x \in \mathbb{R} : x \leq 0\}$$
\end{def.}
\cut
\begin{def.} \textbf{Sets and Set Builder Notation and Hats} \\ 
	A set is a collection of objects. \\ 
	Example: A = The set of all hats. \\ 
	We call the objects in the set elements. \\ 
	Example: A Cowboy hat is a type of hat and thus belongs in the set of all hats: A. \\
	
	Set builder notation is a way of describing a set using mathematical, logical symbols, or words. \\
	Look at the following example: \\ 
	$$E = \{x \in \mathbb{N}: x = 2n \text{ where } n \in \mathbb{N}\}$$
	This reads: $E$ is the set of all x in $\mathbb{N}$ such that $x = 2n$ where $n$ is in $\mathbb{N}$ \\
	This set is also known as the even numbers. \\
	When talking about functions, another common way of describing a set is: 
	$$C_X^Y = \{f : X \to Y | f \text{ is continuous} \}$$
	This reads: $C_X^Y$ is the set of all functions f mapping from $X$ to $Y$ such that $f$ is continuous. 
\end{def.}
\cut
\begin{def.} \textbf{Primitives} \\
	The logical or and not are both primitives and are written: \\ 
	$$\text{logical or: }\lor$$ 
	and 
	$$not: \lnot$$
	Respectively. \\
	Statements are written: $L,M,N,O,P,Q,... etc$ \\
	A statement is a sequence of words or symbols which is either true or false. \\
	So then $L \lor M$ is a new statement composed of $L,\lor,\text{and } M$. \\
	We can then use this as the definition of a new statement: \\ 
	$$N := L \lor M$$
	Which is read: N is defined as L or M. \\
	So N is true if: 
	\begin{itemize}
		\item 	L is true. 
		\item M is true.
		\item L and M are both true. 
	\end{itemize}
	Further, N is false if: 
	L and M are both false. \\ \\
	Similarly, we can define a new statement: $N := \lnot M$ \\ 
	In this case, if M is true then N is false. \\
	In the same vain, if M is false then N is true. 
\end{def.}
\cut
\begin{def.} \textbf{And} \\
	Let $A,B$ be statements. \\ 
	$$A \land B := \lnot((\lnot A) \lor (\lnot B))$$ 
\end{def.}
\cut
\begin{def.} \textbf{Intersection and Union} \\
	Let $A,B$ be sets. \\ 
	$$A \cap B := \{ x : x \in A \land x \in B\}$$
	$$A \cup B := \{ x : x \in A \lor x \in B\}$$
\end{def.}
\cut
\begin{def.} \textbf{Compliment} \\ 
	Let $A$ be a set.  \\
	Then: $x \not \in A := \lnot(x \in A)$ and 
	$A^c := \{x : x \not \in A\}$
\end{def.}
\cut
\begin{axiom} \textbf{Propositional Logic} \\ 
	Let $A,B$ and $C$ be a statements.\\
	Then we have the following axioms: 
	\begin{itemize}
		\item $\vdash A \lor A \Rightarrow A$ 
		\item $\vdash A \Rightarrow A \lor B$ 
		\item $\vdash A \lor B \Rightarrow B \lor A$ 
		\item $\vdash (A \Rightarrow C) \Rightarrow ( A \lor B \Rightarrow B \lor C ) $
	\end{itemize}
	These statements I am taking as fundamentally true and not needing to be proven. 
\end{axiom}
\cut 
\begin{ruleOfInference} \textbf{Modus Ponens} \\
	Let $A,B$ be statements. \\
	If $\vdash A \Rightarrow B$ and $\vdash A$ then $\vdash B$ 
\end{ruleOfInference}
\cut
\begin{def.} \textbf{Logical Implication and Equivalence} \\
	Let $A,B$ be statements: 
	$$A \Rightarrow B := (\lnot A) \lor B$$
	Further: 
	$$A \Leftrightarrow B := (A \Rightarrow B) \wedge (B \Rightarrow A)$$
\end{def.}
\cut
\begin{def.} \textbf{Logical Deduction} \\
	Let $P$ and $Q$ be statements. \\ 
	First, we suppose that $P$ is true. \\
	If we can then show that $Q$ is true, then it is proven that $P \Rightarrow Q$
\end{def.}
\cut
\begin{thm} \textbf{First Theorem} \\ 
	Let $A$ be a statement. \\ 
	Then $A \Rightarrow A \lor A $ \\ 
	\prof
	We know that: \\ 
	$\vdash A \Rightarrow A \lor B$ \hfill [Axiom 2]\\ 
	By replacing B with A we then have: \\ 
	$\vdash A \Rightarrow A \lor A$ \hfill [Principle of Substitution.] 
\end{thm}
\cut
\begin{thm} \textbf{Second Theorem} \\
	Let $A$ be a statement. \\ 
	Then $A \Leftrightarrow A \lor A $ \\ 	
	\prof
	Since $\vdash A \Rightarrow A \lor A$ and $\vdash A \lor A \Rightarrow A$ \hfill [Axiom 1 and the first theorem]\\
	And so we know: \\
	$\vdash A \Leftrightarrow A \lor A$ \hfill [Definition of Equivalence]
\end{thm}
\cut
\begin{thm} \textbf{The Commutative Property of OR} \\
	Let $A,B$ be statements. \\
	Then: $A \lor B \Leftrightarrow B \lor A$ \\ 
	\prof
	By Axiom 3 we know: \\ 
	$\vdash A \lor B \Rightarrow B \lor A$ \hfill [Axiom 3] \\
	We can then Substitute B for C: \\
	$\vdash A \lor C \Rightarrow C \lor A$ \hfill [Principle of Substitution] \\
	Next we substitute A for B: \\ 
	$\vdash B \lor C \Rightarrow C \lor B$ \hfill [Principle of Substitution] \\
	Finally we substitute C for A: \\ 
	$\vdash B \lor A \Rightarrow A \lor B$ \hfill [Principle of Substitution] \\
	And so by definition of equivalence we have that: \\
	$\vdash A \lor B \Leftrightarrow B \lor A$ \hfill [Definition of Equivalence]
\end{thm}
\cut
\begin{thm} \textbf{Shakespeare's theorem} \\ 
	Let $B$ be a statement. \\
	Then $B \lor \lnot B$ \\ 
	\prof
	We have the Axiom: \\
	$\vdash A \Rightarrow A \lor B$ \hfill [Axiom 2] \\ 
	We can then replace A with B and  we have: \\ 
	$\vdash B \Rightarrow B \lor B$ \hfill [Principle of Substitution.] \\ 
	$\vdash \lnot B \lor (B \lor B)$ \hfill [Def. of Implication]\\ 
	$\vdash \lnot B \lor B$ \hfill [Second Theorem] \\ 
	By the commutative property of OR we can then have that: \\ 
	$\vdash B \lor \lnot B$ \hfill [Commutative Property of Or] 
\end{thm}
\cut
\begin{thm} \textbf{The Tautology Theorem} \\ 
	Let $A$ be a statement. \\ 
	Then: $A \Rightarrow A$ and further $A \Leftrightarrow A$ \\
	\prof
	Let $B$ be a statement. \\ 
	From the proof Shakespeare's theorem we know that: \\
	$\vdash \lnot B \lor B$ \hfill [Proof of Shakespeare's theorem.] \\ 
	Replace B with A. \\ 
	$\vdash \lnot A \lor A$ \hfill [Principle of Substitution.] \\
	By the definition of Implication we have that: \\ 
	$\vdash A \Rightarrow A$ \hfill [Def. of Logical Implication] 	
\end{thm}
\cut
\begin{thm} \textbf{The Double Negative Theorem} \\
	Let $A$ be a statement. \\ 
	Then $\lnot(\lnot A) \Leftrightarrow A$ \\
	\prof 
	From Shakespeare's theorem we have: \\ 
	$\vdash B \lor \lnot B$ \hfill [Shakespeare's Theorem] \\ 
	Replace $B$ with $\lnot A$ \\ 
	$\vdash \lnot A \lor \lnot (\lnot A)$ \hfill [Principle of Substitution]\\ 
	Then by the definition of implication we have: \\ 
	$\vdash A \Rightarrow \lnot (\lnot A)$ \hfill [Def. of Logical Implication] \\
	For the next proof we will need the following, substitute $A$ with $\lnot A$ \\ 
	$\vdash \lnot A \Rightarrow \lnot (\lnot (\lnot A))$ \hfill [Principle of Substitution] *1 \\ \\
	Now for the other direction, start with axiom 4: \\ 
	$\vdash (A \Rightarrow C) \Rightarrow ( A \lor B \Rightarrow B \lor C ) $ \hfill [Axiom 4]\\
	Substitute $C$ with $\lnot (\lnot A)$ \\
	$\vdash (A \Rightarrow \lnot (\lnot A)) \Rightarrow (A \lor B \Rightarrow B \lor \lnot (\lnot A) ) $ \hfill [Principle of Substitution]\\
	Substitute $A$ with $\lnot A$ \\ 
	$\vdash (\lnot A \Rightarrow \lnot (\lnot (\lnot A))) \Rightarrow (\lnot A \lor B \Rightarrow B \lor \lnot (\lnot (\lnot A)) ) $ \hfill [Principle of Substitution]\\
	Substitute $B$ with $A$ \\ 
	$\vdash (\lnot A \Rightarrow \lnot (\lnot (\lnot A))) \Rightarrow (\lnot A \lor A \Rightarrow A \lor \lnot (\lnot (\lnot A)) ) $ \hfill [Principle of Substitution]\\
	We have that $\vdash (\lnot A \Rightarrow \lnot (\lnot (\lnot A))) $ from the last part: \\
	$\vdash \lnot A \lor A \Rightarrow A \lor \lnot (\lnot (\lnot A))  $ \hfill [Modus Ponens knowing: *1 ]\\
	We have Shakespeare's theorem and so: \\ 
	$\vdash A \lor \lnot (\lnot (\lnot A))  $ \hfill [Modus Ponens knowing: $\vdash \lnot A \lor A $]\\	
	We know that OR is commutative and so: \\ 
	$\vdash \lnot (\lnot (\lnot A))  \lor A$ \hfill [The Commutative Property of OR]\\
	By the definition of implication we know: \\ 
	$\vdash \lnot (\lnot A) \Rightarrow A$ \hfill [Def. of Logical Implication.]\\
	And finally by the definition of equivalence we know: \\ 
	$\vdash \lnot (\lnot A) \Leftrightarrow A$	\hfill [Def. of Logical Equivalence. ]
\end{thm}
\cut
\begin{thm} \textbf{Contra positive} \\ 
	$(A \Rightarrow B) \Leftrightarrow (\lnot B \Rightarrow \lnot A)$ and $(A \Leftrightarrow B) \Leftrightarrow (\lnot A \Leftrightarrow \lnot B)$ 
	
\end{thm}
\cut 
\begin{thm} \textbf{Captain Morgan's Laws for Logic} \\ 
	Let $A,B$ be statements. \\ 
	Then: $\lnot(A \land B) \Leftrightarrow \lnot A \lor \lnot B$ and $\lnot(A \lor B) \Leftrightarrow \lnot A \land \lnot B$ \\
	\prof
	Using the Deductive method we start with the assumption that: \\
	$\lnot(A \land B)$ \hfill [Deductive Method] \\
	By the definition of and we know the following are equivalent. \\ 
	$ \Leftrightarrow \lnot( \lnot( \lnot A \lor \lnot B) )$ \hfill [Def. And] \\ 
	By the previous theorem we know that: \\ 
	$ \Leftrightarrow \lnot A \lor \lnot B$ \hfill [Double Negative Theorem] \\ 
	Then the Deductive Method tells us that: \\ 
	$\vdash \lnot(A \land B) \Leftrightarrow \lnot A \lor \lnot B$ \hfill [Deductive Method Conclusion] \\
	And so we are done with the first part. \\ \\
	Using the Deductive Method again, we start with the assumption that: \\ 
	$\lnot A \land \lnot B$ \hfill [Deductive Method] \\
	By definition of and, we know that: \\ 
	$\Leftrightarrow \lnot (\lnot (\lnot A) \lor \lnot (\lnot B)) $ \hfill [Def. of And] \\ 
	By the double negative theorem we know that: \\ 
	$\Leftrightarrow \lnot (A \lor B)$ \hfill [Double Negative Theorem] \\
	And so by the Deductive Method we know: \\ 
	$\vdash \lnot A \land \lnot B \Leftrightarrow \lnot (A \lor B)$ \hfill [Conclusion of the Deductive Method]
	Which gives us both statements. 
\end{thm}
\cut
\begin{thm} \textbf{De Morgan's laws} \\
	Let $A,B$ be sets. \\ 
	Then: $(A \cap B)^c = A^c \cup B^c$ and $(A \cup B)^c = A^c \cap B^c$ \\ 
	\prof 
	Let $x \in (A \cap B)^c \Rightarrow x \not \in A \cap B$ \\ 
	$\Rightarrow \lnot(x \in A \cap B)$ \\ 
	$\Rightarrow \lnot(x \in A \land x \in B)$  
\end{thm}
\cut
\begin{def.} \textbf{Subsets} \\ 
	Let $A,B$ be sets. \\ 
	$A \subset B \Leftrightarrow (x \in A \Rightarrow x \in B)$
\end{def.}
\cut
\begin{thm} \textbf{The union only makes things larger} \\
	Let $A,B$ be sets. \\ 
	Then: $A \subset A \cup B$ \\ 
	\prof 
	Let $x \in A$ and $x\in B$ be statements. \\
	Then by Axiom 2 we know: \\ 
	$x \in A \Rightarrow x \in A \lor x \in B$ \hfill [Axiom 2 where the statements are $x \in A$ and $x \in B$] \\ 
	Then by definition of union we know: \\ 
	$x \in A \Rightarrow x \in A \cup B$ \hfill [Def. Union] \\
	And so by definition of subsets we know: \\ 
	$A \subset A \cup B$ \hfill [Def. of Subsets] \\
\end{thm}
\cut
\begin{thm} \textbf{Union and Intersection Distributive Properties} \\ 
	$$A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$$ 	
	$$A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$$
	\prof
	Let $x \in A \cap (B \cup C) \Rightarrow x \in A \land x \in B \cup C$ \\ \\
	Suppose that $x \not \in C \Rightarrow x \in B$ \\ 
	$\Rightarrow x \in A \land x \in B$ \\ 
	$\Rightarrow x \in A \cap B$ \\ 
	$\Rightarrow x \in (A \cap B) \cup (A \cap C) \hfill [(A \cap B) \subset (A \cap B) \cup (A \cap C)]$ \\ \\
	Suppose that $x \not \in B \Rightarrow x \in C$ \\ 
	$\Rightarrow x \in A \text{ and } x \in C$ \\ 
	$\Rightarrow x \in A \cap C$ \\ 
	$\Rightarrow x \in (A \cap B) \cup (A \cap C) \hfill [(A \cap C) \subset (A \cap B) \cup (A \cap C)]$ \\
	Therefore: \\ 
	$$A \cap (B \cup C) \subset (A \cap B) \cup (A \cap C)$$ 	
\end{thm}
\cut
\begin{def.} \textbf{Power Set} \\ 
	Let $X \not = \phi$ \\ 
	$$2^X := \{V : V \subseteq X\}$$
\end{def.}
\cut
\begin{def.} \textbf{Injection, Surjection, and Bijection} \\
	Let $A,B$ be sets and  function $f : A \to B$.
	f is said to be an Injection if: \\ 
	$$(\forall x,y \in A)(f(x) = f(y) \Rightarrow x = y)$$
	f is said to be a Surjection if: \\ 
	$$(\forall y \in B)(\exists x \in A)(f(x) = y)$$
	f is said to be a Bijection if it is both an Injection and a Surjection.  
\end{def.}
\cut
\begin{thm} \textbf{It's pretty big.} \\ 
	Let $X \not = \phi$ and $M := card(X) < card(\mathbb{N})$ \\
	Then: $card(2^X) = 2^M$ 
\end{thm}
\cut
\begin{def.} \textbf{K - Combinations} \\
	Let $S$ be a non empty finite set where $n = card(S)$ \\ 
	A K-combination of S is a subset: $K \subset S$ where $card(K) = k$ \\ 
	We then have the collection: $C(S,k) = \{K \subset S : card(K) = k\}$ \\
	The number of K-combinations = $card(C(S,k)) = \binom{n}{k}$
\end{def.}
\cut
\begin{def.} \textbf{Equivalence Relations} \\
	Let $S \not = \phi$ \\ 
	Then $\approxeq$ is called an Equivalence Relation if: 
	\begin{itemize}
		\item $(\forall a \in S)(a \approxeq a)$ \hfill [Reflexive]
		\item $(\forall a,b \in S)(a \approxeq b \Leftrightarrow b \approxeq a)$ \hfill [Symmetric]
		\item $(\forall a,b,c \in S)(a \approxeq b \wedge b \approxeq c \Rightarrow a \approxeq c)$ \hfill [Transitive] 
	\end{itemize}
\href{https://en.wikipedia.org/wiki/Equivalence_relation}{Reference}
\end{def.}
\cut
\begin{def.} \textbf{Equivalence Class} \\
	Let $S \not = \phi$ and $a \in S$ and $\approxeq$ be an equivalence relation on $S$. \\
	Then the equivalence class $[a]$ is defined as follows: 
		$$[a] := \{x \in S : x \approxeq a\}$$
\end{def.}
\cut
\section{Topology}

\begin{def.} \textbf{Topology} \\
	Let $X \not = \phi$ \\ 
	Further let $\tau \subseteq 2^X$ such that: \\ 
	$$\phi,X \in \tau$$
	$$(\forall A \not = \phi)\left(\{U_\alpha\}_{\alpha \in A} \subseteq \tau \Rightarrow \bigcup_{\alpha \in A} U_\alpha \in \tau\right)$$
	$$(\forall m \in \mathbb{N})\left(\{U_j\}_{j\in \mathbb{N}_m} \Rightarrow \bigcap_{j=1^m} U_j \in \tau\right)$$
\end{def.}
\cut
\begin{def.} \textbf{Relative Topology} \\
	Let $X \not = \phi$ and $Z \subset X$ \\ 
	Then the relative topology on $Z$ is written as follows: \\ 
	$$\tau_Z = \{Z \cap U : U \in \tau_X\}$$
\end{def.}
\cut
\begin{thm} \textbf{The Relative Topology is a Topology on Z} \\
	Let $E \in \tau_Z$ \\
	$$\Rightarrow E = Z \cap U \subset Z$$
	$$\Rightarrow \tau_Z \subseteq 2^Z$$
	And so we have met the first criteria. \\ 
	Next: 
	$$\phi \in \tau \Rightarrow Z \cap \phi \in \tau_Z \Rightarrow Z \cap \phi = \phi \in \tau_Z$$
	Next: 
	$$X \in \tau \Rightarrow Z \cap X \in \tau_Z \Rightarrow Z \cap X = Z \in \tau_Z$$
	Next: 
	Let $A \not = \phi$ and $\{U_\alpha\}_{\alpha \in A} \in \tau_Z$ \\ 
	$$\Rightarrow \exists \{V_\alpha\}_{\alpha \in A} \subset \tau \text{ such that: } U_\alpha = Z \cap V_\alpha$$
	$$\Rightarrow \bigcup_{\alpha \in A} U_\alpha = \bigcup_{\alpha \in A} Z \cap V_\alpha$$
\end{thm}
\cut
\section{Change}
\begin{def.} \textbf{Metric} \\ 
	Let X be a non-empty set.. \\ 
	Let $d : X \times X \to \mathbb{R}^+_0$ such that: 
	\begin{itemize}
		\item $(\forall x,y \in X) d(x,y) = 0 \Leftrightarrow x = y$ 
		\item $(\forall x,y \in X) d(x,y) = d(y,x)$
		\item $(\forall x,y,z \in X) d(x,z) \leq d(x,y) + d(y,z)$
	\end{itemize}	
	Then $d$ is called a metric and $(X,d)$ is called a metric space. \\
	\href{https://en.wikipedia.org/wiki/Metric_space}{Reference}
\end{def.}
\cut
\begin{def.} \textbf{Limit of a function} \\ 
	Let $T : X \to Y$ where $(X,d_X)$ and $(Y,d_Y)$ are metric spaces. \\ 
	Then fix $x_0 \in X$. \\
	If:
	$$(\exists L \in Y)( \forall \epsilon > 0 )(\exists \delta > 0)(\forall x \in X)(d(x,x_0) < \delta \Rightarrow d(f(x),L) < \epsilon)$$
	Then: $$\lim_{x \to x_0} f(x) = L$$ 
	\href{https://en.wikipedia.org/wiki/Limit_of_a_function}{Reference}
\end{def.}
\cut
\begin{def.} \textbf{Derivative} \\ 
	Let $\hat{f} : \mathbb{R} \to \mathbb{R}$ \\ 
	Further let $f = \hat{f}|_U$ where $U \in \tau_\mathbb{R}$ \\ 
	Then $f$ is said to be differentiable at $x \in U$ if there exists an $L_x$ such that: \\ 
	$$L_x = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}$$
	If $L_x$ exists for all $x \in U$ then we write: \\ 
	$$\frac{d}{dx}f(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}$$
	\href{https://en.wikipedia.org/wiki/Differentiable_function}{Reference}
\end{def.}
\cut
\begin{thm} \textbf{Fundamental increment lemma} \\ 
	Let $f$ be described as above and be differentiable at x. \\ 
	Then there exists a function $\phi : \mathbb{R} \to \mathbb{R}$ such that: \\ 
	$$f(x + h) = f(x) + \frac{d}{dx} f(x)h + \phi(x)h$$
	and 
	$$\lim_{h \to 0} \phi(h) = 0$$ 
	\prof 
	Define: $\phi(h) = \frac{f(x+h) - f(x)}{h} - \frac{d}{dx} f(x)$ \\ 
	Then: $\phi(h)h = f(x+h) - f(x) - \frac{d}{dx} f(x)h$ \\ 
	Then: $\phi(h)h + f(x) - \frac{d}{dx} f(x)h = f(x + h)$ \\ 
	And so we have property 1. \\ 
	Next: \\ 
	$$\lim_{h \to 0} \phi(h) = \lim_{h \to 0} \left[\frac{f(x+h) - f(x) - \frac{d}{dx} f(x)h}{h}\right] = \lim_{h \to 0} \left[\frac{f(x+h) - f(x)}{h} - \frac{d}{dx} f(x)\right] $$
	$$= \lim_{h \to 0}\frac{f(x+h) - f(x)}{h} - \lim_{h \to 0} \frac{d}{dx} f(x) = \frac{d}{dx} f(x) - \frac{d}{dx} f(x) = 0$$
\end{thm}
\cut
\begin{def.} \textbf{Partial Derivative} \\
	Let $\hat{f} : \mathbb{R}^n \to \mathbb{R}$ \\
	Further let $f = \hat{f}|_U$ where $U \in \tau_{\mathbb{R}^n}$ \\ 
	Then $f$ is said to be differentiable at $x \in U$ with respect to the i'th component of $x$ if there exists an $L_{x_i}$ such that: \\  
	$$L_{x_i} = \lim_{h \to 0} \frac{f(x_1,...,x_i + h,...,x_n) - f(x_1,...,x_i,...,x_n)}{h}$$
	If $L_{x_i}$ exists for all $x \in U$ then we write: \\ 
	$$\frac{\partial}{\partial x_i} f(x) = \lim_{h \to 0} \frac{f(x_1,...,x_i + h,...,x_n) - f(x_1,...,x_i,...,x_n)}{h}$$ 
	\href{https://en.wikipedia.org/wiki/Partial_derivative}{Reference}
\end{def.}
\cut
\begin{thm} \textbf{Equivalent characterization} \\ 
	Let $\hat{f} : \mathbb{R}^n \to \mathbb{R}$ \\
	Further let $f = \hat{f}|_U$ where $U \in \tau_{\mathbb{R}^n}$ \\ 
	And let $f$ be differentiable at $x \in U$ with respect to the i'th component of $x$, then: 
	$$L_{x_i} = \lim_{h \to 0} \frac{f(x_1,...,x_i + h,...,x_n) - f(x_1,...,x_i,...,x_n)}{h}$$
	$$\Leftrightarrow 0 = \lim_{h \to 0} \left[\frac{f(x_1,...,x_i + h,...,x_n) - f(x_1,...,x_i,...,x_n)}{h} - L_{x_i}\right]$$
	$$\Leftrightarrow 0 = \lim_{h \to 0} \left[\frac{f(x_1,...,x_i + h,...,x_n) - f(x_1,...,x_i,...,x_n)}{h} - \frac{L_{x_i} \cdot h}{h}\right]$$
	$$\Leftrightarrow 0 = \lim_{h \to 0} \left[\frac{f(x_1,...,x_i + h,...,x_n) - f(x_1,...,x_i,...,x_n) - \langle L_{x_i} , h \rangle}{h}  \right]$$	
\end{thm}
\cut
\begin{def.} \textbf{Gradient} \\ 
	Let $\hat{f} : \mathbb{R}^n \to \mathbb{R}$ and let $f : U \to \mathbb{R}$ such that $f = \hat{f}|_U$ where $U \in \tau_{\mathbb{R}^n}$ \\ 
	$f$ is said to be differentiable at $x \in U$ if $\exists \nabla f : \mathbb{R}^n \to \mathbb{R}^n$ such that: \\  	
	$$\lim_{h \to 0}\frac{|f(x + h) - f(x) - \langle \nabla f(x),h \rangle|}{||h||} = 0$$ 
\end{def.} 
\cut
\begin{thm} \textbf{Form of the Gradient} \\ 
	Let $f$ be defined as above. \\ 
	Then $\nabla f: \mathbb{R}^n \to \mathbb{R}^n$ where: \\ 
	\[
	\nabla f(x) = 
	\begin{bmatrix}
	\frac{\partial }{\partial x_1}f(x) \\ 
	\vdots \\
	\frac{\partial }{\partial x_n}f(x) \\ 				
	\end{bmatrix}
	\forall x \in \mathbb{R}^n
	\]
	is the form of $\nabla f$ which satisfies the above statement if $f$ is differentiable. \\
	\href{https://en.wikipedia.org/wiki/Gradient}{Reference} \\ \\
	\prof 
	Suppose $\nabla f$ is defined as above and all the partial derivatives exist. \\ 
	Then: 
	$$\frac{1}{||h||} |f(x + h) - f(x) - \langle \nabla f(x),h \rangle| = \frac{1}{||h||} \left|f(x + h) - f(x) - \sum_{j=1}^n \frac{\partial}{\partial x_j} f(x)\cdot h_j\right|$$
\end{thm}
\cut
\newpage
\begin{def.} \textbf{Matrix Functional Differentiability} \\ 
	Let $\hat{T} : \mathbb{R}^{n \times m} \to \mathbb{R}$ and let $T : U \to \mathbb{R}$ such that $T = \hat{T}|_U$ where $U \in \tau_{\mathbb{R}^{n \times m}}$ \\ 
	T is said to be differentiable at $x \in U$ if $\exists D : \mathbb{R}^{n \times m} \to \mathbb{R}^{n \times m}$ such that: \\  
	$$\lim_{h \to 0}\frac{|T(x + h) - T(x) - \langle D T(x),h \rangle|}{||h||} = 0$$
	where $\langle \cdot, \cdot \rangle$ is an inner product defined on $\mathbb{R}^{n \times m}$
\end{def.}
\cut
\begin{def.} \textbf{Frobenius inner product} \\
	The Frobenius inner product is defined as: \\ 
	$$\langle \cdot , \cdot \rangle_{FB} : \mathbb{R}^{n \times m} \times \mathbb{R}^{n \times m} \to \mathbb{R} \text{ such that: } \langle A,B \rangle_{FB} = \sum_{i=1}^n \sum_{j=1}^m a_{i,j}b_{i,j} \text{ for all } A,B \in \mathbb{R}^{n \times m}$$
\end{def.}
\cut
\begin{thm} \textbf{Form of Matrix Functional Derivative} \\
	\[
	DT(x) = 
	\begin{bmatrix}
	\frac{\partial}{\partial x_{1,1}} & \cdots & \frac{\partial}{\partial x_{1,m}} \\ 
	\vdots & \ddots & \vdots \\
	\frac{\partial}{\partial x_{n,1}} & \cdots & \frac{\partial}{\partial x_{n,m}} \\ 
	\end{bmatrix}	
	\]


\end{thm}
\cut

\newpage
\begin{def.} \textbf{Differentiability of a multi-variable function.} \\
	Let $\hat{f} : \mathbb{R}^m \to \mathbb{R}^n$ such that: \\ 
	\[
	\hat{f}(x) = 
	\begin{bmatrix}
	f_1(x) \\ 
	\vdots \\
	f_n(x) \\
	\end{bmatrix}
	\text{ and } 
	(\forall j \in \mathbb{N}_n)(f_j : \mathbb{R}^m \to \mathbb{R})
	\]
	Further let $f = \hat{f}|_U$ where $U \in \tau_{\mathbb{R}^m}$ \\ 
	Then $f$ is said to be differentiable at $x \in U$ if there exists a linear operator $J_{f(x)}: \mathbb{R}^m \to \mathbb{R}^n$ such that: \\ 
	$$\lim_{h \to \vec{0}} \frac{||f(x + h) - f(x) + J_{f(x)}(h)||_{\mathbb{R}^m}}{||h||_{\mathbb{R}^n}} = 0$$
	\href{https://en.wikipedia.org/wiki/Differentiable_function}{Reference}
\end{def.}
\cut
\begin{thm} \textbf{If a multi-variable function, f, is differentiable at x then the linear operator J is the Jacobian matrix.} \\ 
	So our guess is that: \\ 
	\[
	J_{f(x)} = 
	\begin{bmatrix}
	\frac{\partial}{\partial x_1} f_1(x) & \cdots & \frac{\partial}{\partial x_n} f_1(x) \\ 
	\vdots & \ddots & \vdots \\ 
	\frac{\partial}{\partial x_1} f_m(x) & \cdots & \frac{\partial}{\partial x_n} f_m(x) \\ 
	\end{bmatrix}
	\]
	since this form is a linear operator mapping from the appropriate space to the appropriate space. It should be noted that the transpose of this matrix can not satisfy the definition of differentiability of a multi-variable function and so it is not the correct linear operator. 
\end{thm}
\cut
\begin{def.} \textbf{Matrix operator differentiability} \\ 
	Let $T : \mathbb{R}^{n\times m} \to \mathbb{R}^n$ such that: \\
	\[
	T(A) = 
	\begin{bmatrix}
	T_1(A) \\ 
	\vdots \\ 
	T_n(A) \\ 	
	\end{bmatrix}
	\forall A \in \mathbb{R}^{n\times m}
	\text{ and } (\forall j \in \mathbb{N}_n)(T_j : \mathbb{R}^{n \times m} \to \mathbb{R})
	\]
	Then T is said to be differentiable at $A \in \mathbb{R}^{n \times m}$ if there exists a linear operator $D : \mathbb{R}^{n \times m} \to \mathbb{R}^n$ where: \\ 
	$$\lim_{h \to 0} \frac{||T(A + h) - T(A) + D(h)||_{\mathbb{R}^n}}{||h||_{\mathbb{R}^{n\times m}}} = 0$$
\end{def.}
If $D$ exists then it is called the Matrix operator derivative and is written: $D_{\mathbb{R}^{n \times m}}T(A)$ \\ 
\cut
\begin{thm} \textbf{The form of the Matrix operator derivative.} \\ 
	Let $T$ be described as above and differentiable at $A \in \mathbb{R}^{n \times m}$ \\ 
	\[
	\frac{T(A + h) - T(A)}{||h||} = 
	\begin{bmatrix}
	\frac{T_1(A + h) - T_1(A)}{||h||} \\ 
	\vdots \\ 
	\frac{T_n(A + h) - T_n(A)}{||h||} \\ 	
	\end{bmatrix}
	\]
	and so: \\ 
	\[
	\lim_{h \to 0} \frac{T(A + h) - T(A)}{||h||} = 
	\begin{bmatrix}
	\lim_{h \to 0} \frac{T_1(A + h) - T_1(A)}{||h||} \\ 
	\vdots \\ 
	\lim_{h \to 0} \frac{T_n(A + h) - T_n(A)}{||h||} \\ 	
	\end{bmatrix}
	\]	
\end{thm}
\cut
\newpage
\begin{def.} \textbf{Subspace Differentiability} \\ 
	Let $X = \{X_j\}_{j=1}^{n}$ be a sequence of finite dimensional vector spaces where $dim(X_j) = k_j = m_j \times n_j$ \\ 
	Let $T : \prod_{j=1}^{n} X_j \to Y$ where $Y$ is a finite dimensional vector space with $dim(Y) = k_y$ \\ 
	Let $x_j \in X_j$ for some $j \in \mathbb{N}_n$ \\ 
	Where 
	\[
	x_j = 
	\begin{bmatrix}
	x_{1,1} & \cdots & x_{1,n_j} \\ 
	\vdots & \ddots & \vdots \\
	x_{m_j,1} & \cdots & x_{m_j,n_j} \\ 	
	\end{bmatrix}
	\]
	$T$ is said to be differentiable at $x \in X$ where $x = (x_0,...,x_j,...,x_{n-1})$ with respect to $X_j$ if there exists a linear operator $D : X_j \to Y$: \\ 
	Given $h \in X_j \setminus \{\vec{0}\}$ define $\hat{h} = (0,...,h,...,0) \in X \text{ where } h$ is in the j'th place of $\hat{h}$: \\
	$$\lim_{h \to 0} \frac{||T(x + \hat{h}) - T(x) + D(h)||_{X}}{||h||_{X_j}} = 0$$
	Then $D$ is called the subspace derivative of $T$ at x with respect to $X_j$ and is written: $D_{x_j} T(x)$ \\ 
\end{def.}
\cut

\newpage
\begin{def.} \textbf{Product space Derivative} \\ 
		Let $X = \{X_j\}_{j=0}^{n-1}$ be a sequence of finite dimensional vector spaces where $dim(X_j) = k_j$ \\ 
		Let $T : \prod_{j=0}^{n-1} X_j \to Y$ where $Y$ is a finite dimensional vector space with $dim(Y) = k_y$ \\
		Let $\{x_j\}_{j=0}^{n-1}$ be a sequence of vectors such that: $(\forall j \in \{0,...,n-1\})(x_j \in X_j)$ \\ 
		The product space derivative at the point $z \in X$ is: \\ 
		\[
		D_{X} T(z) = 
		\begin{bmatrix}
		D_{x_0} T(z) \\ 
		\vdots \\ 
		D_{x_{n-1}} T(z) \\ 		
		\end{bmatrix}
		\]
\end{def.}
\cut

\newpage
\begin{def.} \textbf{Fréchet derivative} \\ 
		Let $V,W$ be normed vector spaces and $U \subset V$ be an open set. \\ 
		An operator $f : U \to W$ is said to be Fréchet differentiable if there exists a bounded linear operator $A : V \to W$ such that: \\ 
		$$\lim_{||h||\to0} \frac{||f(x+h) - f(x) + Ah||_W}{||h||_V} = 0$$
		\href{https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative}{Reference}
\end{def.} 
\cut
\begin{thm} \textbf{Fréchet derivative of a bounded linear operator} \\ 
	Let $V,W$ be normed vector spaces and $U \subset V$ be an open set. \\ 
	Let $\hat{f} : V \to W$ be a bounded linear operator. \\
	Then lets look at $f = \hat{f}|_{U}$ \\ 
	My guess is that $A = \hat{f}$ \\ 
	Let $x \in U$ and $h \in U \pitchfork ||h|| \not = 0$ and $x+h \in U$, Then: \\ 
	$$\frac{||f(x+h) - f(x) + Ah||_W}{||h||_V} = \frac{||f(x)+f(h) - f(x) + \hat{f}(h)||_W}{||h||_V} = \frac{||f(x)+f(h) - f(x) + f(h)||_W}{||h||_V}= 0$$
	Thus let $\epsilon > 0$ and $\delta > 0$ \\ 
	Then if $0 < ||h|| < \delta$ we know that  
	$\frac{||f(x+h) - f(x) + Ah||_W}{||h||_V} = 0 < \epsilon$ \\ 
	Therefore: 
	$$\lim_{||h||\to0} \frac{||f(x+h) - f(x) + Ah||_W}{||h||_V} = 0$$
	Thus $A = \hat{f}$ is the Fréchet derivative  of f. 
\end{thm}
\cut
\subsection{Finite Composition Operator}
\begin{def.} \textbf{Finite Composition Operator} \\ 
	Let the collection $X = \{X_j\}_{j=0}^{n}$ be a finite sequence of sets. \\ 
	Further let $\{T_j\}_{j=0}^{n-1}$ be a finite sequence of operators such that $(\forall j \in \mathbb{N}_{n-1})(T_j : X_j \to X_{j+1})$ \\ 
	Then $T^n : X_0 \to X_{n}$ defined by: 
	$$T^n := \bigcirc_{j=0}^{n-1} T_j$$ 
	is called the \textbf{Finite Composition Operator defined on X}. \\ 
\end{def.}
\cut
\begin{def.} \textbf{Multi-variable Finite Composition Iteration} \\ 
	Let the collection $X = \{X_j\}_{j=0}^n$ and $Y = \{Y_j\}_{j=0}^{n-1}$ be finite sequences of sets. \\ 
	Further let $\{T_j\}_{j=0}^{n-1}$ be a finite sequence of operators such that: $(\forall j \in \mathbb{N}_{n-1})(T_j : X_j \times Y_j \to X_{j+1})$ \\
	Let $T^n : X_0 \times \prod_{j=0}^{n-1}Y_j \to X_n$ where: \\ 
	$$T^n (x,y) = z_n \text{ where } z_{j+1} = T_j( z_{j},\pi_j (y) ) \text{ or } z_{j+1} = T_j( z_{j} )  \text{ and } z_0 = x \in X_0$$
\end{def.}
\cut
\begin{def.} \textbf{Gradient Descent} \\ 
Let $E : \mathbb{R}^n \to \mathbb{R}$ be a differentiable operator. \\ 
The method of Gradient Descent says that a local minimum of $E$ can be found using the following iteration: \\ 
$$a_{n+1} = a_n - \gamma \nabla E(a_n)$$
Where $\gamma > 0$ \\ 
\end{def.}
\cut
\begin{exmp} \textbf{Objective Operator for Data Set Defined Operator Approximation} \\ 		
Let $X \subset \mathbb{R}^n, Y \subset \mathbb{R}^m$ such that $X\times Y$ defines an operator T. \\ 
$$E(a) = \sum_{x \in X} ||T(x) - T^n(x,a)||$$ 
\end{exmp}
\cut
\newpage
\section{Surjective Continuous Non-decreasing Bounded Functionals}
	Let $B = \{f : \mathbb{R} \to [0,1] | f \text{ is surjective, continuous, and non-decreasing.}\}$ \\ 
	\begin{thm} \textbf{B is convex. } \\
			Let $f,g \in B$ and $h(x) := \lambda f(x) + (1-\lambda)g(x)$ where $\lambda \in [0,1]$ \\ 
			Then $h$ is still continuous since the linear combination of continuous functions is continuous.  \\ \\
			\prof
			Since both f and g are surjective and non-decreasing, then there exists $x_0,y_0,x_1,y_1$ in $\mathbb{R}$ such that: \\ 
			$f(x_0) = 0 = g(y_0)$ and $f(x_1) = 1 = g(y_1)$ \\ 
			Suppose WLOG that $x_0 \leq y_0$ and $x_1 \leq y_1$ \\
			Then we know that: 
			$$h(x_0) = \lambda f(x_0) + (1-\lambda)g(x_0) = \lambda 0 + (1-\lambda)0 = 0$$
			and 
			$$h(y_1) = \lambda f(y_1) + (1-\lambda)g(y_1) = \lambda 1 + (1-\lambda)1 = 1$$
			Now if we pick $\alpha \in [0,1]$ by the intermediate value theorem, we know that there exists an $x_\alpha \in [x_0,y_1]$ such that: 
			$$h(x_\alpha) = \alpha$$
			Since $\alpha$ was arbitrary element, I have shown that $h$ is surjective. \\ \\
			Finally, let $x_0 < x_1$ be elements in $\mathbb{R}$ \\ 
			Then we know that $f(x_0) \leq f(x_1)$ and $g(x_0) \leq g(x_1)$ \\ 
			$\Rightarrow \lambda f(x_0) \leq \lambda f(x_1)$ and $(1-\lambda)g(x_0) \leq (1-\lambda)g(x_1)$ \\ 
			$\Rightarrow \lambda f(x_0) + (1-\lambda)g(x_0) \leq \lambda f(x_1) + (1-\lambda)g(x_1)$ \\ 
			$\Rightarrow h(x_0) \leq h(x_1)$ \\ 
			Thus $h$ is non-decreasing. \\ 
			Since h is surjective, continuous, and non-decreasing, then $h \in B$ \\ 
			Thus $B$ is convex. 
	\end{thm}
	\cut
	\begin{thm} \textbf{B is translation invariant.} \\
			Let $f \in B$ and $g(x) := f(x+c)$ where $c \in \mathbb{R}$ \\
			f is continuous and so is the addition operator so g is continuous. \\
			\prof
			Let $\alpha \in [0,1]$ since f is surjective then $\exists x \in \mathbb{R} \pitchfork f(x) = \alpha$ \\ 
			Then $g(x-c) = f(x+c-c) = f(x) = \alpha$ and so g is surjective. \\ 
			Let $x < y$ be elements in $\mathbb{R}$ \\
			Then $f(x) \leq f(y) \Rightarrow f(x+c) \leq f(y+c)$ \\ 
			$\Rightarrow g(x) \leq f(y)$ and so g is non-decreasing. \\ 
			Thus $g \in B$ and B is therefore translation invariant. 
	\end{thm}
	\cut
	\begin{thm} \textbf{B is not complete.} \\
			
	\end{thm}
	\cut
	\begin{thm} \textbf{Every element in B can be decomposed as a finite non-trivial convex combination from B} \\
	
	\end{thm}
	\cut
	\newpage
	\section{A topological description of finite metric spaces.}
	Finite spaces are of interest because they describe the world of computers. This being the case, we would still like to do analysis on these spaces and analysis starts with topological descriptions. \\
	\cut
	\begin{thm} \textbf{The First Rule of Induced Topologies on Finite Metric Subspaces is:} \\
		Let $V \subset X$ where $0 < card(V) =: N < card(\mathbb{N})$ and $X$ is a metric space. \\
		Then the subspace topology on $V$ is the discrete topology. \\ 
		\prof
		The associated topological space on V is: 
		$$\tau_V = \{V \cap U : U \in \tau_X\}$$
		Since V is of finite carnality, we can uniquely number each element. \\ 
		Thus:
		$$V = \bigcup_{i=1}^N \{v_i\}$$
		Further: 
		$$V_{\min} := \min\{d(x,y) : x,y \in V\} $$
		Let $v \in V$ and $\epsilon_V = \frac{V_{\min}}{2}$ \\ 
		Then $B(v,\epsilon_V) \in \tau_X \Rightarrow V \cap B(v,\epsilon_V) \in \tau_V$ \\ 
		However $V \cap B(v,\epsilon_V)  = \{v\}$ and since $v$ was arbitrary, we thus know that: $(\forall v \in V)(\{v\} \in \tau_V)$ \\
		We can now prove that $\tau_V = 2^V$ \\ 
		By definition we know that $\tau_V \subset 2^V$ \\ 
		Let $E \in 2^V$ then $E = \bigcup_{j=1}^M \{v_j\}$ where $M \leq N$ \\ 
		Since we know ever $\{v_j\}$ is open we know that $E$ is open and thus: $E \in \tau_V$ \\ 
		\textbf{Thus the induced topology on a finite subset of a Metric space is the discrete topology. }		
	\end{thm}
	\cut
	\begin{lma} \textbf{Everything is Continuous when your domain is finite.} \\ 
			Let $V \subset X$ and $Y \not = \phi$ where $X,Y$ are metric spaces and $0 < card(V) = N < card(\mathbb{N})$ \\
			Then ever $f : V \to Y$ is continuous. \\ 
			\prof
			Let $U \in \tau_W$ \\ 
			Then: $f^{-1}(U) \subset V$ where $f : V \to Y$ is arbitrary. \\
			Thus: $f^{-1}(U) \in \tau_V$ by the previous theorem. \\
			And so $f$ is continuous. \\
			\textbf{And so every $f : V \to Y$ is continuous. }
	\end{lma}
	\cut
	\begin{thm} \textbf{Everything is Lipschitz continuous when your domain is finite.} \\
		So if we want to have some form of meaningful topological description of continuity for these spaces, we are going to need a "stronger" form of continuity. \\
		Let $V \subset X$ and $Y \not = \phi$ where $X,Y$ are metric spaces and $0 < card(V) = N < card(\mathbb{N})$ \\
		Further let $f : V \to Y$, then $f$ is \textbf{Lipschitz continuous}. \\
		\prof
		We can simply look at: \\
		$$K := \max\left( \left\{\frac{d_Y(f(x),f(y))}{d_X(x,y)} : x,y \in V \right\} \right)$$
		Then we know that: 
		$$(\forall x,y \in V)\left( \frac{d_Y(f(x),f(y))}{d_X(x,y)} \leq K\right)$$
		And therefore: \\ 
		$$(\forall x,y \in V)\left( d_Y(f(x),f(y)) \leq Kd_X(x,y)\right)$$
		\textbf{And thus $f$ is Lipschitz continuous.} \\ 
	\end{thm}
	\cut
	\begin{def.} \textbf{K - Families} \\ 
		Let $V \subset X$ and $Y \not = \phi$ where $X,Y$ are Normed spaces and $0 < card(V) = N < card(\mathbb{N})$ \\
		First we have the set of all operators.  
		$$\Lambda := \{f | f : V \to Y\}$$
		Next we have the $K - Families$ 
		$$\Lambda(K) := \{f \in \Lambda | K \text{ is the smallest Lipschitz constant for f.}\}$$
		The set: $\Lambda(K)$ is called a K - Family. 
		Next we have the $K - Nests$ 
		$$B(K) := \{f : V \to Y | (\forall x,y \in V) (||f(x) - f(y)|| \leq K ||x - y||)\}$$
	\end{def.}
	\cut
	\begin{thm} \textbf{K - Properties} 
		\begin{itemize}
			\item $B(K) = \bigcup_{L \in [0,K]} \Lambda(L)$ \\
				\prof
				Let $f \in B(K) \Rightarrow f \text{ is lipschitz.}$ \\
				Which means that there exists a unique smallest constant $K'$ where: $0 \leq K' \leq K$ \\
				Thus $f \in \Lambda(K') \subset \bigcup_{L \in [0,K]} \Lambda(L)$ \\ 
				Therefore: $B(K) \subseteq \bigcup_{L \in [0,K]} \Lambda(L)$ \\ \\
				Let $f \in \bigcup_{L \in [0,K]} \Lambda(L) \Rightarrow \exists L \in [0,K] \pitchfork (\forall x,y \in V)(||f(x)-f(y)|| \leq L||x - y|| \leq K||x - y||)$ \\ 
				$\Rightarrow f \in B(K)$ \\
				$\Rightarrow \bigcup_{L \in [0,K]} \Lambda(L) \subseteq B(K)$ \\ 
				$\therefore B(K) = \bigcup_{L \in [0,K]} \Lambda(L)$ 
			\item $L \leq K \Rightarrow B(L) \subseteq B(K)$ \\
				\prof
				Let $f \in B(L) \text{ and } x,y \in V$ \\
				$\Rightarrow ||f(x) - f(y)|| \leq L ||x - y|| \leq K ||x - y||)$ \\ 
				$\Rightarrow f \in B(K)$ 
			\item $B(K) \text{ is convex.}$ \\
				\prof
				Let $f,g \in B(K)$ and $h_\lambda(x) = \lambda f(x) + (1-\lambda) g(x)$ for some $\lambda \in [0,1]$ \\
				Further let $x,y \in V$: \\
				$$||h_\lambda(x) - h_\lambda(y)|| = ||\lambda f(x) + (1-\lambda) g(x) - (\lambda f(y) + (1-\lambda) g(y))|| =  ||\lambda f(x) + (1-\lambda)g(x) - \lambda f(y) - (1-\lambda)g(y) ||$$
				$$ = ||\lambda(f(x) - f(y)) + (1-\lambda)(g(x) - g(y))|| \leq \lambda||(f(x) - f(y))|| + (1-\lambda)||(g(x) - g(y))||$$
				$$\leq \lambda K||x-y|| + (1 - \lambda) K||x - y|| = K||x - y||$$						
				$\Rightarrow h_\lambda \in B(K)$
			\item $\Lambda(K)$ is not convex. \\
				\prof
				Let $K > 0$ \\ 
				If $f \in \Lambda(K) \Rightarrow -f \in \Lambda(K)$ \\ 
				However $\frac{f + -f}{2} = 0 \in \Lambda(0) \not = \Lambda(K)$ \\   
		\end{itemize}
	\end{thm}
	\cut
	\begin{lma} \textbf{Paths through Function spaces} \\ 
		Let $f \in \Lambda(K)$ and $g \in \Lambda(L)$ then $h_\lambda(x) := \lambda f(x) + (1-\lambda) g(x)$ at most belongs to the family: $\Lambda(\lambda K + (1-\lambda) L)$
		Further: $H : [0,1] \to \Gamma $ where $H(\lambda) = h_\lambda$ is a path through the function space:
		$$\Gamma = \bigcup_{\nu \in [0,1]} \Lambda(\nu \cdot \max(L,K))$$
		\prof
		$$||h(x) - h(y)|| = ||\lambda f(x) + (1-\lambda) g(x) - (\lambda f(y) + (1-\lambda) g(y))|| =  ||\lambda f(x) + (1-\lambda)g(x) - \lambda f(y) - (1-\lambda)g(y) ||$$
		$$ = ||\lambda(f(x) - f(y)) + (1-\lambda)(g(x) - g(y))|| \leq \lambda||(f(x) - f(y))|| + (1-\lambda)||(g(x) - g(y))||$$
		$$\leq \lambda K||x-y|| + (1 - \lambda) L||x - y|| = (\lambda K + (1 - \lambda) L)||x - y|| $$		
	\end{lma}
	\cut
	\begin{thm} \textbf{Ordering} \\ 
		For this theorem, we will need both the domain and co-domain to be finite. \\
		First, Let $V \subset X$ and $W \subset Y$ where $X,Y$ are normed spaces. \\ 
		Next let $\Lambda := \{f | f : V \to W\}$ where $0 < card(V) =: N < card(\mathbb{N})$ and $0 < card(W) =: M < card(\mathbb{N})$ \\ 
		And: $\Lambda(K) := \{f \in \Lambda | K \text{ is the Lipschitz constant for f.}\}$ as before. \\ 
		Then: 
		$$K < L \Rightarrow card(\Lambda(K)) < card(\Lambda(L))$$ 
	\end{thm}
	\cut

	\section{Convex Operators} 
			\begin{thm} \textbf{Invariance under Convex Monotonic Operator Composition} \\
				\href{https://en.wikipedia.org/wiki/Convex_function}{Reference}
			\end{thm}
	\cut
			\begin{thm} \textbf{Invariance under Affine Composition} \\
					Let $f: \mathbb{R}^m \to \mathbb{R}^n$ be a convex operator, then: \\ 
					$$g(x) = f(Ax + b)$$
					Is also a convex operator. \\  
			\end{thm}
	\cut
\end{document}